# 多元线性回归

## 定义

在学习多元线性回归之前，首先了解其定义

【百度词条相关介绍】**在回归分析中，如果有两个或两个以上的自变量，就称为多元回归。**

引入一个例子(引自CSDN)eg.1

| 房屋面积(m^2) | 租凭价格(x1000) |
| ------------- | --------------- |
| 10            | 0.8             |
| 15            | 1.0             |
| 20            | 1.8             |
| 30            | 2               |
| 50            | 3.2             |

对于上述数据集，我们通过分析可以得出价格与面积呈线性增长的关系，我们可以用一条直线来拟合所有的样本点，即是
```math
y=ax+b
```
那么，当数据集维度增加时，我们同样可以对数据进行分析，用一个超平面来拟合所有的样本点，例如eg.2

| Number | Tv    | Radio | Newspaper | Sales |
| ------ | ----- | ----- | --------- | ----- |
| 1      | 230.1 | 37.8  | 69.2      | 22.1  |
| 2      | 44.5  | 39.3  | 45.1      | 10.4  |
| 3      | 17.2  | 45.9  | 69.3      | 9.3   |
| 4      | 151.5 | 41.3  | 58.5      | 18.5  |
| 5      | 180.8 | 10.8  | 58.4      | 12.9  |

对于该数据集，我们可以用一个超平面来拟合所有的样本点，即
```math
h(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}
```
那么推广至二元及二元以上的自变量，我们便得到了多元线性回归模型，也可以写成矩阵相乘的形式
```math
h(x)=\sum_{i=0}^{N} \theta_{i}x_{i}=\theta^{T}X(其中N表示自变量的数量)
```

于是，对于每一个样本点都有：

```math
y_{i}=\theta^{T}x_{i}+\epsilon_{i}
```

其中，公式的第一项$`\theta^{T}x_{i}`$表示的是预测值$`\hat{y}`$，而$`\epsilon_{i}`$表示的是误差值，由于假设样本之间是相互独立的，那么误差变量随机产生，所以服从正态分布，于是我们假设$`\epsilon`$服从均值为0，方差为$`\sigma^{2}`$的高斯分布。



## 解析解推导

根据正态分布的概率密度函数我们可以得到：

```math
p(\epsilon_{i})=\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(\epsilon_{i})^{2}}{2\sigma^{2}})} 
```

将y代入函数得到：

```math
p(y_{i}|x_{i};\theta )=\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})} 
```

将各项联乘，我们得到了关于$`\theta`$的似然函数：

```math
L(\theta)=\prod_{i=1}^{m}\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})}  
```

将联乘变换成累加：

```math
ln(L(\theta))=ln(\prod_{i=1}^{m}\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})})
```

```math
=\sum_{i=1}^{m}ln(\frac{1}{\sigma \sqrt{2\pi}}e^{(-\frac{(y_{i}-\theta^{T}x_{i})^{2}}{2\sigma^{2}})})
```

```math
=m\times ln(\frac{1}{\sigma \sqrt{2\pi}})-\frac{1}{2\sigma ^{2}}\sum_{i=1}^{m}(y_{i}-\theta^{T}x_{i})^{2}
```

要让似然函数L取最大值，观察函数

```math
J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(y_{i}-\theta^{T}x_{i})^{2}
```

我们需要求得函数J的最小值，而当J取最小值时，满足条件：

```math
\frac{\partial J(\theta)}{\partial \theta} =0
```

即其导数值为0，对函数$`J(\theta)`$的自变量$`\theta`$求导，于是我们得到：

```math
J(\theta)=\frac{1}{2} \sum_{i=1}^{m}(y_{i}-\theta^{T}x_{i})^{2}
```

```math
=\frac{1}{2}(X\theta-Y)^{T}(X\theta-Y)······(写成矩阵表达式)
```

```math
=\frac{1}{2}(\theta^{T}X^{T}-Y^{T})(X\theta-Y)
```

```math
=\frac{1}{2}(\theta^{T}X^{T}X\theta-\theta^{T}X^{T}Y-Y^{T}X\theta+Y^{T}Y)
```

```math
于是\frac{\partial J(\theta)}{\partial \theta} =\frac{1}{2}(2X^{T}X\theta-X^{T}Y-Y^{T}X)=0
```

```math
X^{T}X\theta=X^{T}Y
```

```math
得到解析解为：\theta=(X^{T}X)^{-1}X^{T}Y
```



## 梯度下降法

